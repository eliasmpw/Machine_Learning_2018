{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from custom_helpers import *\n",
    "from plot import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x, y, flag_method, degree, lambda_=0, gamma=1.e-6, max_iters=1000, k_fold=int(5), seed=143225):\n",
    "    \"Train the model and evaluate loss based on cross validation\"\n",
    "    mses_tr = []\n",
    "    mses_te = []\n",
    "    accuracy_tr = []\n",
    "    accuracy_te = []\n",
    "    \n",
    "    flag_add_offset = True\n",
    "    flag_standardize = True\n",
    "    flag_remove_outliers = False\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed);\n",
    "    for i in range(k_fold):\n",
    "        newk_index = np.delete(k_indices, i, 0)\n",
    "        indices_train = newk_index.ravel()\n",
    "        indices_test = k_indices[i]\n",
    "\n",
    "        # Train data at each iteration \"i\" of the loop\n",
    "        x_train = x[indices_train]\n",
    "        y_train = y[indices_train]\n",
    "\n",
    "        # Validate the data at each iteration \"i\" of the loop\n",
    "        x_test = x[indices_test]\n",
    "        y_test = y[indices_test]\n",
    "\n",
    "        # Prepare data (Standardisation and offset)\n",
    "        training_tx, testing_tx = prepare_data(x_train, x_test, flag_add_offset, flag_standardize, flag_remove_outliers, degree)\n",
    "        \n",
    "        # create initial w for methods using it\n",
    "        initial_w = np.zeros(training_tx.shape[1])\n",
    "\n",
    "        if flag_method == 0:\n",
    "            # Use linear regression (full gradient descent)\n",
    "            weight, _ = least_squares_GD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 1:\n",
    "            # Use linear regression (stochastic gradient descent)\n",
    "            weight, _ = least_squares_SGD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 2:\n",
    "            # Use least squares method\n",
    "            weight, _ = least_squares(y_train, training_tx)\n",
    "            \n",
    "        if flag_method == 3:\n",
    "            # Use ridge regression\n",
    "            weight, _ = ridge_regression(y_train, training_tx, lambda_)\n",
    "            \n",
    "        if flag_method == 4:\n",
    "            # Use logistic regression\n",
    "            weight, _ = logistic_regression(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 5:\n",
    "            # Use regularized logistic regression\n",
    "            weight, _ = reg_logistic_regression(y_train, training_tx, initial_w, max_iters, gamma, lambda_)\n",
    "            \n",
    "        loss_te = np.sqrt(2 * compute_mse(y_test, testing_tx, weight))\n",
    "        loss_tr = np.sqrt(2 * compute_mse(y_train, training_tx, weight))\n",
    "        \n",
    "        # Append loss of this round to list\n",
    "        mses_tr.append(loss_tr)\n",
    "        mses_te.append(loss_te)\n",
    "        \n",
    "        # calculate accuracy and add it to list\n",
    "        y_pred_tr = predict_labels(weight, training_tx)\n",
    "        y_pred_te = predict_labels(weight, testing_tx)\n",
    "        accuracy_tr.append(np.sum(y_pred_tr == y_train)/len(y_train))\n",
    "        accuracy_te.append(np.sum(y_pred_te == y_test)/len(y_test))\n",
    "\n",
    "\n",
    "    mean_accuracy_tr = np.mean(accuracy_tr)\n",
    "    mean_accuracy_te = np.mean(accuracy_te)\n",
    "    loss_tr = np.mean(mses_tr)\n",
    "    loss_te = np.mean(mses_te)\n",
    "    return loss_tr, loss_te, mean_accuracy_tr, mean_accuracy_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Main\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data, please wait\n",
      "Data loaded, continue!!\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "print(\"Loading Data, please wait\")\n",
    "train_y, train_x, ids_train = load_csv_data('data/train.csv')\n",
    "print(\"Data loaded, continue!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMethods mapping\\n0    Linear regression (full gradient descent)\\n1    Linear regression (stochastic gradient descent)\\n2    Least squares method\\n3    Ridge regression\\n4    Logistic regression (stochastic gradient descent)\\n5    Regularized logistic regression (stochastic gradient descent)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Methods mapping\n",
    "0    Linear regression (full gradient descent)\n",
    "1    Linear regression (stochastic gradient descent)\n",
    "2    Least squares method\n",
    "3    Ridge regression\n",
    "4    Logistic regression (stochastic gradient descent)\n",
    "5    Regularized logistic regression (stochastic gradient descent)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation for one set of parameters only\n",
    "Get the RMSE for one method with defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7fdbeb8a4b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain_cross_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For the Degree: %d cross-validation loss is %f, and accuracy is %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-dd01ce77d804>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(x, y, flag_method, degree, lambda_, gamma, max_iters, k_fold, seed)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Prepare data (Standardisation and offset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtraining_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_add_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_standardize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_remove_outliers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# create initial w for methods using it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Machine_Learning_2018/custom_helpers.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(x_train, x_test, flag_add_offset, flag_standardize, flag_remove_outliers, degree)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflag_remove_outliers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# replace the outliers with the most common element of each column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Machine_Learning_2018/custom_helpers.py\u001b[0m in \u001b[0;36mremove_outliers\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mreplace_most_frecuent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mupper_quartile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mlower_quartile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mIQR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupper_quartile\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlower_quartile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlower_quartile\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mIQR\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_quartile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIQR\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ml/ENV/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3538\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Percentiles must be in the range [0, 100]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3539\u001b[0m     return _quantile_unchecked(\n\u001b[0;32m-> 3540\u001b[0;31m         a, q, axis, out, overwrite_input, interpolation, keepdims)\n\u001b[0m\u001b[1;32m   3541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ml/ENV/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3650\u001b[0m     r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,\n\u001b[1;32m   3651\u001b[0m                     \u001b[0moverwrite_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3652\u001b[0;31m                     interpolation=interpolation)\n\u001b[0m\u001b[1;32m   3653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ml/ENV/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ml/ENV/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3752\u001b[0m         \u001b[0mweights_above\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3754\u001b[0;31m         \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_below\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_above\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3756\u001b[0m         \u001b[0;31m# ensure axis with qth is first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 2;\n",
    "degree = 2\n",
    "lambda_ = 0.001\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.01\n",
    "max_iters = 20\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain=remove_invalid(train_x)\n",
    "\n",
    "_, loss_te, _, accuracy_te = cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, lambda_, gamma, max_iters)\n",
    "print(\"For the Degree: %d cross-validation loss is %f, and accuracy is %f\" %(degree, loss_te, accuracy_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for methods without regularisation\n",
    "Test polynominal expansion of different degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 4;\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.1\n",
    "max_iters = 1000\n",
    "\n",
    "# Define range for the polynomial expansion\n",
    "degree_range = np.arange(1, 3)\n",
    "\n",
    "train_losses = np.zeros(len(degree_range))\n",
    "test_losses = np.zeros(len(degree_range))\n",
    "train_accuracies = np.zeros(len(degree_range))\n",
    "test_accuracies = np.zeros(len(degree_range))\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain=remove_invalid(train_x)\n",
    "\n",
    "for ind_degree, degree in enumerate(degree_range):\n",
    "    loss_tr, loss_te , accuracy_tr, accuracy_te= cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, 0, gamma, max_iters)\n",
    "    print(\"For the Degree: %d , The LOSS is : %f\" %(degree, loss_te))\n",
    "    train_losses[ind_degree] = loss_tr\n",
    "    test_losses[ind_degree] = loss_te\n",
    "    test_accuracies[ind_degree] = accuracy_tr\n",
    "    test_accuracies[ind_degree] = accuracy_te\n",
    "    \n",
    "print(\"Cross Validation finished!!\")\n",
    "best_value = np.unravel_index(np.argmin(test_losses), test_losses.shape)\n",
    "print(\"The best degrees are: \", degree_range[best_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Test accuracy:\")\n",
    "print(test_accuracies)\n",
    "cross_validation_visualization_degree(degree_range, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for Methods using regularisation\n",
    "Grid search over different degrees of polynominal expansion and for different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6244300482999456\n",
      "iteration\t 100 \tloss:  0.5947400919348644\n",
      "iteration\t 150 \tloss:  0.5820373274626097\n",
      "iteration\t 200 \tloss:  0.5728809514233614\n",
      "iteration\t 250 \tloss:  0.5658750402261199\n",
      "iteration\t 300 \tloss:  0.5604126331420326\n",
      "iteration\t 350 \tloss:  0.5560955680418571\n",
      "iteration\t 400 \tloss:  0.552643123927952\n",
      "iteration\t 450 \tloss:  0.54985257884781\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6245079087298614\n",
      "iteration\t 100 \tloss:  0.5950044158532545\n",
      "iteration\t 150 \tloss:  0.5824356514361658\n",
      "iteration\t 200 \tloss:  0.5733888099619017\n",
      "iteration\t 250 \tloss:  0.566475757295157\n",
      "iteration\t 300 \tloss:  0.5610924159225454\n",
      "iteration\t 350 \tloss:  0.556842633195852\n",
      "iteration\t 400 \tloss:  0.5534473917493695\n",
      "iteration\t 450 \tloss:  0.5507054623527218\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6247309791715705\n",
      "iteration\t 100 \tloss:  0.5953271991580401\n",
      "iteration\t 150 \tloss:  0.5828083152621322\n",
      "iteration\t 200 \tloss:  0.5737909752899786\n",
      "iteration\t 250 \tloss:  0.5668935926620068\n",
      "iteration\t 300 \tloss:  0.5615180583398546\n",
      "iteration\t 350 \tloss:  0.5572719664531806\n",
      "iteration\t 400 \tloss:  0.5538782857681408\n",
      "iteration\t 450 \tloss:  0.5511367828624677\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6243497146737671\n",
      "iteration\t 100 \tloss:  0.5947045624867114\n",
      "iteration\t 150 \tloss:  0.5820445840832458\n",
      "iteration\t 200 \tloss:  0.5729336914987154\n",
      "iteration\t 250 \tloss:  0.5659726072431465\n",
      "iteration\t 300 \tloss:  0.5605513894467062\n",
      "iteration\t 350 \tloss:  0.5562703629960359\n",
      "iteration\t 400 \tloss:  0.5528483246982817\n",
      "iteration\t 450 \tloss:  0.550082738916251\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.624740661707601\n",
      "iteration\t 100 \tloss:  0.5951854945685351\n",
      "iteration\t 150 \tloss:  0.5825106153583675\n",
      "iteration\t 200 \tloss:  0.5733813222837236\n",
      "iteration\t 250 \tloss:  0.5664062266373994\n",
      "iteration\t 300 \tloss:  0.5609753549397323\n",
      "iteration\t 350 \tloss:  0.5566883366555754\n",
      "iteration\t 400 \tloss:  0.5532633068497474\n",
      "iteration\t 450 \tloss:  0.550497078282443\n",
      "For the Degree: 1 and lambda 0.00E+00, The LOSS is : 1.048543\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6247519917007667\n",
      "iteration\t 100 \tloss:  0.5951932004186123\n",
      "iteration\t 150 \tloss:  0.5824894564990535\n",
      "iteration\t 200 \tloss:  0.5734617092350994\n",
      "iteration\t 250 \tloss:  0.5666778004856888\n",
      "iteration\t 300 \tloss:  0.5614928925268234\n",
      "iteration\t 350 \tloss:  0.557483385612811\n",
      "iteration\t 400 \tloss:  0.5543509648356976\n",
      "iteration\t 450 \tloss:  0.5518808961464566\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6248237388427959\n",
      "iteration\t 100 \tloss:  0.5954441328015566\n",
      "iteration\t 150 \tloss:  0.5828726256235622\n",
      "iteration\t 200 \tloss:  0.5739540403435515\n",
      "iteration\t 250 \tloss:  0.56726253545605\n",
      "iteration\t 300 \tloss:  0.5621557205119442\n",
      "iteration\t 350 \tloss:  0.5582118346202253\n",
      "iteration\t 400 \tloss:  0.5551342651657056\n",
      "iteration\t 450 \tloss:  0.5527098724755848\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6250486841482771\n",
      "iteration\t 100 \tloss:  0.5957695636029087\n",
      "iteration\t 150 \tloss:  0.5832456736885102\n",
      "iteration\t 200 \tloss:  0.5743538266109566\n",
      "iteration\t 250 \tloss:  0.5676758021274052\n",
      "iteration\t 300 \tloss:  0.5625750611609753\n",
      "iteration\t 350 \tloss:  0.5586333863548643\n",
      "iteration\t 400 \tloss:  0.5555560838690257\n",
      "iteration\t 450 \tloss:  0.5531310098761106\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6246690019631314\n",
      "iteration\t 100 \tloss:  0.5951522895549206\n",
      "iteration\t 150 \tloss:  0.5824906137207168\n",
      "iteration\t 200 \tloss:  0.5735080357252619\n",
      "iteration\t 250 \tloss:  0.5667683644991666\n",
      "iteration\t 300 \tloss:  0.5616233869770295\n",
      "iteration\t 350 \tloss:  0.5576480261295909\n",
      "iteration\t 400 \tloss:  0.5545437915175403\n",
      "iteration\t 450 \tloss:  0.552096408712193\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6250577651692211\n",
      "iteration\t 100 \tloss:  0.5956314772649507\n",
      "iteration\t 150 \tloss:  0.5829572612381069\n",
      "iteration\t 200 \tloss:  0.5739580284079566\n",
      "iteration\t 250 \tloss:  0.5672059154028021\n",
      "iteration\t 300 \tloss:  0.5620530515897593\n",
      "iteration\t 350 \tloss:  0.5580734757349486\n",
      "iteration\t 400 \tloss:  0.5549676755869658\n",
      "iteration\t 450 \tloss:  0.5525205430291126\n",
      "For the Degree: 1 and lambda 3.00E+02, The LOSS is : 1.047874\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6250629045254009\n",
      "iteration\t 100 \tloss:  0.5956820253060126\n",
      "iteration\t 150 \tloss:  0.5830857986509163\n",
      "iteration\t 200 \tloss:  0.5742720176861659\n",
      "iteration\t 250 \tloss:  0.5677614035472149\n",
      "iteration\t 300 \tloss:  0.5628797315614684\n",
      "iteration\t 350 \tloss:  0.559185738551979\n",
      "iteration\t 400 \tloss:  0.5563690859663469\n",
      "iteration\t 450 \tloss:  0.5542070405547952\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6251288914265184\n",
      "iteration\t 100 \tloss:  0.5959207748146442\n",
      "iteration\t 150 \tloss:  0.5834550579733433\n",
      "iteration\t 200 \tloss:  0.5747494891577654\n",
      "iteration\t 250 \tloss:  0.5683299730573044\n",
      "iteration\t 300 \tloss:  0.5635244431638994\n",
      "iteration\t 350 \tloss:  0.5598934233848474\n",
      "iteration\t 400 \tloss:  0.5571283500745833\n",
      "iteration\t 450 \tloss:  0.5550082116417552\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6253556082369256\n",
      "iteration\t 100 \tloss:  0.5962485144244761\n",
      "iteration\t 150 \tloss:  0.5838283886183716\n",
      "iteration\t 200 \tloss:  0.5751473755559843\n",
      "iteration\t 250 \tloss:  0.5687396914685471\n",
      "iteration\t 300 \tloss:  0.5639389806779388\n",
      "iteration\t 350 \tloss:  0.5603091815078756\n",
      "iteration\t 400 \tloss:  0.5575436115413194\n",
      "iteration\t 450 \tloss:  0.5554222469042528\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6249774246075522\n",
      "iteration\t 100 \tloss:  0.5956361820684796\n",
      "iteration\t 150 \tloss:  0.5830811998636529\n",
      "iteration\t 200 \tloss:  0.5743119403567283\n",
      "iteration\t 250 \tloss:  0.5678447115171406\n",
      "iteration\t 300 \tloss:  0.5630018011268678\n",
      "iteration\t 350 \tloss:  0.5593405127906393\n",
      "iteration\t 400 \tloss:  0.5565503994164706\n",
      "iteration\t 450 \tloss:  0.5544092345271031\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6253640957298239\n",
      "iteration\t 100 \tloss:  0.5961134253293842\n",
      "iteration\t 150 \tloss:  0.5835474002539426\n",
      "iteration\t 200 \tloss:  0.5747624757725435\n",
      "iteration\t 250 \tloss:  0.5682836522940578\n",
      "iteration\t 300 \tloss:  0.5634336389965766\n",
      "iteration\t 350 \tloss:  0.5597687216637258\n",
      "iteration\t 400 \tloss:  0.5569773670681931\n",
      "iteration\t 450 \tloss:  0.5548365431570298\n",
      "For the Degree: 1 and lambda 6.00E+02, The LOSS is : 1.049442\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6253619628932113\n",
      "iteration\t 100 \tloss:  0.5961761260927048\n",
      "iteration\t 150 \tloss:  0.5837293087329958\n",
      "iteration\t 200 \tloss:  0.5751544300333193\n",
      "iteration\t 250 \tloss:  0.5689295346140987\n",
      "iteration\t 300 \tloss:  0.5643559538337939\n",
      "iteration\t 350 \tloss:  0.5609767398151041\n",
      "iteration\t 400 \tloss:  0.5584708385994083\n",
      "iteration\t 450 \tloss:  0.5566087952850934\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6254225462153734\n",
      "iteration\t 100 \tloss:  0.5964040843156789\n",
      "iteration\t 150 \tloss:  0.5840866147434496\n",
      "iteration\t 200 \tloss:  0.5756190443489093\n",
      "iteration\t 250 \tloss:  0.5694836088157963\n",
      "iteration\t 300 \tloss:  0.5649836529575583\n",
      "iteration\t 350 \tloss:  0.5616641058122711\n",
      "iteration\t 400 \tloss:  0.5592058622036442\n",
      "iteration\t 450 \tloss:  0.5573813801122123\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6256509219818035\n",
      "iteration\t 100 \tloss:  0.5967337120738029\n",
      "iteration\t 150 \tloss:  0.5844598293674549\n",
      "iteration\t 200 \tloss:  0.576014861135759\n",
      "iteration\t 250 \tloss:  0.5698898361205172\n",
      "iteration\t 300 \tloss:  0.5653936727267974\n",
      "iteration\t 350 \tloss:  0.5620746064153826\n",
      "iteration\t 400 \tloss:  0.5596153673172011\n",
      "iteration\t 450 \tloss:  0.5577893682741507\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6252741590188492\n",
      "iteration\t 100 \tloss:  0.5961259285868212\n",
      "iteration\t 150 \tloss:  0.5837197924623032\n",
      "iteration\t 200 \tloss:  0.5751889197546332\n",
      "iteration\t 250 \tloss:  0.5690066483819894\n",
      "iteration\t 300 \tloss:  0.5644707297860304\n",
      "iteration\t 350 \tloss:  0.5611227708389352\n",
      "iteration\t 400 \tloss:  0.5586416552669177\n",
      "iteration\t 450 \tloss:  0.5567985152250189\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6256588339694591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 100 \tloss:  0.5966011916342429\n",
      "iteration\t 150 \tloss:  0.5841850624673914\n",
      "iteration\t 200 \tloss:  0.5756391721763219\n",
      "iteration\t 250 \tloss:  0.5694458306292465\n",
      "iteration\t 300 \tloss:  0.5649032472991616\n",
      "iteration\t 350 \tloss:  0.5615519725071738\n",
      "iteration\t 400 \tloss:  0.5590697958578755\n",
      "iteration\t 450 \tloss:  0.5572270620886409\n",
      "For the Degree: 1 and lambda 9.00E+02, The LOSS is : 1.050647\n",
      "Cross Validation finished!!\n",
      "(0, 1)\n",
      "Best degree: 1, with lambda 300.000000 \n"
     ]
    }
   ],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 5;\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.1\n",
    "max_iters = 500\n",
    "\n",
    "# Define range for the polynomial expansion and for lambda\n",
    "degree_range = np.arange(1, 2)\n",
    "lambda_range = np.arange(0, 1000, 300)\n",
    "\n",
    "train_losses_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "test_losses_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "train_accuracies_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "test_accuracies_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain = remove_invalid(train_x)\n",
    "\n",
    "for ind_degree, degree in enumerate(degree_range):\n",
    "    for ind_lambda_, lambda_ in enumerate(lambda_range):\n",
    "        loss_tr, loss_te , accuracy_tr, accuracy_te= cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, lambda_, gamma, max_iters)\n",
    "        print(\"For the Degree: %d and lambda %.2E, The LOSS is : %f\" %(degree, lambda_, loss_te))\n",
    "        train_losses_matrix[ind_degree, ind_lambda_] = loss_tr\n",
    "        test_losses_matrix[ind_degree, ind_lambda_] = loss_te\n",
    "        train_accuracies_matrix[ind_degree, ind_lambda_] = accuracy_tr\n",
    "        test_accuracies_matrix[ind_degree, ind_lambda_] = accuracy_te\n",
    "\n",
    "print(\"Cross Validation finished!!\")\n",
    "best_value = np.unravel_index(np.argmin(test_losses_matrix), test_losses_matrix.shape)\n",
    "print(best_value)\n",
    "print(\"Best degree: %d, with lambda %f \" %(degree_range[best_value[0]], lambda_range[best_value[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:\n",
      "[[0.707696 0.70772  0.70772  0.707636]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize\n",
    "print(\"Test accuracy:\")\n",
    "print(test_accuracies_matrix)\n",
    "cross_validation_visualization_lambda(lambda_range, train_losses_matrix[0, :], test_losses_matrix[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
